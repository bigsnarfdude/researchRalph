# researchRalph Progress Log

Started: 2026-01-19
Completed: 2026-01-20
Experiments: 100

## Final Result

**Best probe: exp094_elu = 0.8342 test AUROC (+0.13 vs baseline)**

Architecture:
```python
Binary+Log features (32-dim) → Linear(32, 16) → ELU → Linear(16, 1)
```

## Patterns (Key Learnings)

### What Works
1. **Binary features** - Presence/absence generalizes better than raw magnitude
2. **Log transform** - Compresses outliers, helps generalization
3. **Smooth activations** - ELU/GELU/LeakyReLU >> ReLU (avoids dead neurons)
4. **Small hidden layers** - 4-16 units sufficient, more = overfitting
5. **BatchNorm** - Helps generalization (+0.11)
6. **Clipping outliers** - Values > 10 are noise

### What Doesn't Work
1. **Attention mechanisms** - Overfit badly (0.44)
2. **Deep MLPs** - Overfit even with dropout
3. **Raw features** - Magnitude alone is noisy
4. **Max-only aggregation** - Loses signal (0.51)
5. **Heavy dropout** (>0.5) - Hurts more than helps
6. **Normalized counts** - Loses information

### Feature Importance
- **Layer 40 > Layer 53** (0.78 vs 0.65 when used alone)
- Single feature L40_F12574 alone gets 0.71
- L53 features add noise, not signal

---

## Experiment Log

### Baseline (EXP-000)
- 4-stat aggregation → Linear = 0.70 test AUROC

### Phase 1: Feature Engineering (EXP-001 to 025)
- Binary features: +0.07
- Log transform: +0.05
- Binary+Log combo: +0.09 (0.79)
- Dropout: marginal help

### Phase 2: Architecture Search (EXP-026 to 050)
- BatchNorm: +0.11
- LayerNorm: worse
- Tanh squash: +0.09
- Skip connections: helped

### Phase 3: Activation Functions (EXP-091 to 098)
- ELU: **+0.13** (BEST)
- GELU: +0.13
- LeakyReLU: +0.13
- SELU: +0.09
- ReLU: +0.06 (dead neuron problem)

### Phase 4: Ablations (EXP-045 to 070)
- L40 only: 0.78
- L53 only: 0.65
- Single feature: 0.71
- All 8 features: best

---

## Conclusions

1. **Probe architecture matters less than feature representation**
   - Binary+log features are the key insight
   - Linear probe on binary+log already gets 0.79

2. **Hidden layer + smooth activation pushes to 0.83**
   - Small nonlinearity helps capture feature interactions
   - Must use smooth activation (ELU/GELU) to avoid dead neurons

3. **Still not at 0.85 target**
   - Remaining gap is likely data distribution shift
   - gold_106 is different from training data
   - May need more diverse training data

4. **For production: use exp094_elu**
   - 0.83 AUROC, 30 TP, 21 FN, 6 FP, 49 TN
   - Simple, fast, interpretable
