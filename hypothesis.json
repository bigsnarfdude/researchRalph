{
  "research_question": "What probe architecture maximizes alignment faking detection AUROC on gold_106, given 8 SAE features from layers 31/40/53?",
  "constraints": {
    "features": "8 SAE features: L40_F12574, L40_F8921, L40_F15484, L53_F15529, L53_F8003, L53_F4824, L53_F351, L31_F15111",
    "training_data": "vincentoh/alignment-faking-training (training_data_final.json + hard_negatives.json)",
    "test_data": "gold_106.json - NEVER train on this, eval only",
    "max_seq_len": 512
  },
  "baseline": {
    "method": "4-stat aggregation (max, mean, count, std) per feature → 32-dim → Linear",
    "val_auroc": 0.86,
    "test_auroc": 0.70,
    "probe_file": "probes/baseline.py"
  },
  "current_best": {
    "experiment_id": "EXP-011",
    "method": "Binary (presence/absence) + Log-transformed magnitude per feature",
    "test_auroc": 0.79,
    "delta": "+0.09",
    "probe_file": "probes/exp011_binary_log.py"
  },
  "experiments_run": 25,
  "key_findings": [
    "Binary features (presence/absence) generalize better than raw magnitude",
    "Log transform compresses outliers and helps generalization",
    "Layer 40 features more predictive than Layer 53 (0.78 vs 0.65)",
    "Single feature L40_F12574 alone achieves 0.71 AUROC",
    "Attention mechanisms overfit on small data",
    "MLPs overfit even with dropout",
    "Heavy dropout (>0.5) hurts more than helps"
  ],
  "failed_directions": [
    "Attention pooling (0.44)",
    "MLP with dropout (0.61)",
    "Max-only aggregation (0.51)",
    "Normalized binary counts (0.57)"
  ],
  "promising_directions": [
    "Binary + Log combination",
    "Layer 40 feature focus",
    "Simple linear probes with regularization"
  ]
}
