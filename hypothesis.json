{
  "phase": "ARCHITECTURE_COMPLETE",
  "next_phase": "FEATURE_DISCOVERY",

  "research_question": "What probe architecture maximizes alignment faking detection AUROC on gold_106, given 8 SAE features from layers 31/40/53?",
  "constraints": {
    "features": "8 SAE features: L40_F12574, L40_F8921, L40_F15484, L53_F15529, L53_F8003, L53_F4824, L53_F351, L31_F15111",
    "training_data": "vincentoh/alignment-faking-training",
    "test_data": "gold_106.json (106 samples, 51 AF)"
  },

  "baseline": {
    "method": "4-stat aggregation → Linear",
    "test_auroc": 0.70
  },

  "current_best": {
    "experiment_id": "exp094_elu",
    "method": "Binary+Log features → 16-unit hidden + ELU → Linear",
    "val_auroc": 0.8956,
    "test_auroc": 0.8342,
    "delta": "+0.1342",
    "probe_file": "probes/exp094_elu.py",
    "confusion": {"tp": 30, "fn": 21, "fp": 6, "tn": 49}
  },

  "experiments_run": 100,

  "validated_findings": {
    "architecture": {
      "binary_features": "+0.07 AUROC (presence/absence generalizes)",
      "log_transform": "+0.02 AUROC (compresses outliers)",
      "elu_activation": "+0.05 AUROC (avoids dead neurons)",
      "small_hidden": "16 units sufficient, more overfits"
    },
    "features": {
      "semantic_vs_random": "+0.33 AUROC (0.83 vs 0.51)",
      "semantic_vs_statistical": "+0.36 AUROC (0.83 vs 0.47)",
      "all_8_contribute": "No redundant features (all positive importance)"
    },
    "ablation_study": {
      "most_important": [
        {"feature": "L40_F15484 (Future self)", "delta": -0.080, "rank": 1},
        {"feature": "L53_F8003 (Training context)", "delta": -0.053, "rank": 2},
        {"feature": "L53_F4824 (Compliance)", "delta": -0.048, "rank": 3}
      ],
      "least_important": [
        {"feature": "L40_F8921 (Self-preservation)", "delta": -0.015, "rank": 8},
        {"feature": "L40_F12574 (RLHF awareness)", "delta": -0.021, "rank": 7}
      ],
      "insight": "L40 not uniformly best - F15484 critical, F8921/F12574 least important"
    },
    "iterative_vs_grid": "23.5x more sample efficient (4 vs 94 experiments)"
  },

  "top_10": [
    {"id": "exp094_elu", "test": 0.8342, "method": "Binary+log + ELU hidden"},
    {"id": "exp098_gelu", "test": 0.8324, "method": "Binary+log + GELU hidden"},
    {"id": "exp096_leaky", "test": 0.8321, "method": "Binary+log + LeakyReLU hidden"},
    {"id": "exp090_residual", "test": 0.8196, "method": "Binary+log + residual"},
    {"id": "exp097_swish", "test": 0.8160, "method": "Binary+log + SiLU hidden"},
    {"id": "exp092_wide", "test": 0.8135, "method": "Binary+log + 64-unit hidden"},
    {"id": "exp044_batchnorm", "test": 0.8128, "method": "Binary+log + BatchNorm"},
    {"id": "exp091_deep", "test": 0.8109, "method": "Binary+log + 3-layer MLP"},
    {"id": "exp093_tiny", "test": 0.8057, "method": "Binary+log + 4-unit hidden"},
    {"id": "exp053_clip", "test": 0.8021, "method": "Clipped features + 4-stat"}
  ],

  "key_findings": [
    "Binary+log features + hidden layer + smooth activation = 0.83 AUROC",
    "ELU, GELU, LeakyReLU all work well (avoid ReLU dead neurons)",
    "Small hidden layer (4-16 units) sufficient, larger doesn't help",
    "Random features = 0.51 AUROC (chance) - semantic selection essential",
    "L40_F15484 (Future self) is most important feature (-0.08 when removed)",
    "All 8 features contribute positively - no redundant features",
    "L53 features collectively important (Training context, Compliance, XML)"
  ],

  "next_phase_hypotheses": [
    "Find more features like L40_F15484 (future self concern)",
    "Semantic clusters (awareness/reasoning/action) improve coverage",
    "Correlation-based expansion around L40_F15484 finds good features",
    "L40_F8921/F12574 could be replaced with more predictive alternatives"
  ],

  "status": "ARCHITECTURE_COMPLETE - ready for FEATURE_DISCOVERY phase"
}
