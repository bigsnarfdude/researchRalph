{
  "research_question": "What probe architecture maximizes alignment faking detection AUROC on gold_106, given 8 SAE features from layers 31/40/53?",
  "constraints": {
    "features": "8 SAE features: L40_F12574, L40_F8921, L40_F15484, L53_F15529, L53_F8003, L53_F4824, L53_F351, L31_F15111",
    "training_data": "vincentoh/alignment-faking-training",
    "test_data": "gold_106.json (106 samples, 51 AF)"
  },
  "baseline": {
    "method": "4-stat aggregation → Linear",
    "test_auroc": 0.70
  },
  "current_best": {
    "experiment_id": "exp094_elu",
    "method": "Binary+Log features → 16-unit hidden + ELU → Linear",
    "val_auroc": 0.8956,
    "test_auroc": 0.8342,
    "delta": "+0.1342",
    "probe_file": "probes/exp094_elu.py",
    "confusion": {"tp": 30, "fn": 21, "fp": 6, "tn": 49}
  },
  "experiments_run": 100,
  "top_10": [
    {"id": "exp094_elu", "test": 0.8342, "method": "Binary+log + ELU hidden"},
    {"id": "exp098_gelu", "test": 0.8324, "method": "Binary+log + GELU hidden"},
    {"id": "exp096_leaky", "test": 0.8321, "method": "Binary+log + LeakyReLU hidden"},
    {"id": "exp090_residual", "test": 0.8196, "method": "Binary+log + residual"},
    {"id": "exp097_swish", "test": 0.8160, "method": "Binary+log + SiLU hidden"},
    {"id": "exp092_wide", "test": 0.8135, "method": "Binary+log + 64-unit hidden"},
    {"id": "exp044_batchnorm", "test": 0.8128, "method": "Binary+log + BatchNorm"},
    {"id": "exp091_deep", "test": 0.8109, "method": "Binary+log + 3-layer MLP"},
    {"id": "exp093_tiny", "test": 0.8057, "method": "Binary+log + 4-unit hidden"},
    {"id": "exp053_clip", "test": 0.8021, "method": "Clipped features + 4-stat"}
  ],
  "key_findings": [
    "Binary+log features + hidden layer + smooth activation = 0.83 AUROC",
    "ELU, GELU, LeakyReLU all work well (avoid ReLU dead neurons)",
    "Small hidden layer (4-16 units) sufficient, larger doesn't help",
    "Layer 40 features more predictive than Layer 53",
    "Clipping outliers helps (exp053)",
    "BatchNorm helps generalization (exp044)",
    "Attention mechanisms overfit",
    "Deep MLPs overfit without careful regularization"
  ],
  "status": "COMPLETE - achieved 0.83 AUROC (target was 0.85)"
}
